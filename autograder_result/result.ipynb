{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad81a6d-1fed-499c-9c95-fb69e89a7f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:15.653901Z",
     "iopub.status.busy": "2023-11-22T17:39:15.653323Z",
     "iopub.status.idle": "2023-11-22T17:39:18.225049Z",
     "shell.execute_reply": "2023-11-22T17:39:18.223848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\r\n",
      "=======================\r\n",
      "Status=Up/Down\r\n",
      "|/ State=Normal/Leaving/Joining/Moving\r\n",
      "--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \r\n",
      "UN  172.22.0.2  187.94 KiB  16      55.5%             71e62da2-1e9a-4bab-ae1d-d0e8f61e9422  rack1\r\n",
      "UN  172.22.0.3  70.27 KiB   16      70.6%             bfbc6286-3efa-43c7-9770-8069a33a0a9f  rack1\r\n",
      "UN  172.22.0.4  70.28 KiB   16      73.9%             f9d5afc9-45ca-4a78-a5e8-4cf93d6a6686  rack1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a20b2c-e91a-43ec-b803-5ab403442b99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:18.231450Z",
     "iopub.status.busy": "2023-11-22T17:39:18.230669Z",
     "iopub.status.idle": "2023-11-22T17:39:19.007942Z",
     "shell.execute_reply": "2023-11-22T17:39:19.004378Z"
    }
   },
   "outputs": [],
   "source": [
    "#Connect to the Cassandra cluster\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d45fbfa-9090-4390-aede-7cd3289acfb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:19.015856Z",
     "iopub.status.busy": "2023-11-22T17:39:19.015043Z",
     "iopub.status.idle": "2023-11-22T17:39:24.721519Z",
     "shell.execute_reply": "2023-11-22T17:39:24.720248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE weather.stations (\n",
      "    id text,\n",
      "    date date,\n",
      "    name text static,\n",
      "    state text,\n",
      "    record station_record,\n",
      "    PRIMARY KEY (id, date)\n",
      ") WITH CLUSTERING ORDER BY (date ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND cdc = false\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND memtable = 'default'\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND extensions = {}\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "#drop a weather keyspace if it already exists\n",
    "cass.execute(\"drop keyspace if exists weather;\")\n",
    "#create a weather keyspace with 3x replication\n",
    "cass.execute(\"create keyspace weather with replication={'class': 'SimpleStrategy', 'replication_factor': 3};\")\n",
    "#inside weather, create a station_record type containing two ints: tmin and tmax\n",
    "cass.execute(\"\"\"\n",
    "CREATE TYPE\n",
    "weather.station_record (\n",
    "    tmin int,\n",
    "    tmax int\n",
    ")\n",
    "\"\"\")\n",
    "#inside weather, create a stations table\n",
    "cass.execute(\"\"\"\n",
    "CREATE TABLE weather.stations (\n",
    "    id text,\n",
    "    name TEXT STATIC,\n",
    "    date date,\n",
    "    record station_record,\n",
    "    state TEXT,\n",
    "    PRIMARY KEY (id,date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")\n",
    "\n",
    "print(cass.execute(\"describe table weather.stations \").one().create_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c14cbff-ca2d-47d4-88fa-52dd1125fd3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:24.738145Z",
     "iopub.status.busy": "2023-11-22T17:39:24.735649Z",
     "iopub.status.idle": "2023-11-22T17:39:41.168015Z",
     "shell.execute_reply": "2023-11-22T17:39:41.166308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-77b47c0b-71ba-4d9e-a30e-f07101cfa8cd;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.typesafe#config;1.4.1 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (141ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (150ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (328ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (53ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (26ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (143ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (39ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (100ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (35ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (31ms)\n",
      ":: resolution report :: resolve 5334ms :: artifacts dl 1313ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-77b47c0b-71ba-4d9e-a30e-f07101cfa8cd\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/107ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 17:39:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c0e3fe-3ad3-49e8-b1be-ae0e4d8194a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:41.175220Z",
     "iopub.status.busy": "2023-11-22T17:39:41.173888Z",
     "iopub.status.idle": "2023-11-22T17:39:41.181794Z",
     "shell.execute_reply": "2023-11-22T17:39:41.180487Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext # for interacting directly with RDDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748c6acf-3a0e-4247-85f6-73e09d4e1da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:41.187465Z",
     "iopub.status.busy": "2023-11-22T17:39:41.186552Z",
     "iopub.status.idle": "2023-11-22T17:39:41.192613Z",
     "shell.execute_reply": "2023-11-22T17:39:41.191017Z"
    }
   },
   "outputs": [],
   "source": [
    "#q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a67866-082b-46af-acb6-5a19b2085c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:41.197495Z",
     "iopub.status.busy": "2023-11-22T17:39:41.196821Z",
     "iopub.status.idle": "2023-11-22T17:39:47.033520Z",
     "shell.execute_reply": "2023-11-22T17:39:47.031888Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\"ghcnd-stations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b03d9d1-8142-4b66-aad7-329ad064b966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:47.043467Z",
     "iopub.status.busy": "2023-11-22T17:39:47.040583Z",
     "iopub.status.idle": "2023-11-22T17:39:54.667159Z",
     "shell.execute_reply": "2023-11-22T17:39:54.665598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:=============================>                             (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pandas_df=df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1295e142-505e-4ee9-b25e-38e11402cb91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:54.675049Z",
     "iopub.status.busy": "2023-11-22T17:39:54.673917Z",
     "iopub.status.idle": "2023-11-22T17:39:54.753742Z",
     "shell.execute_reply": "2023-11-22T17:39:54.752227Z"
    }
   },
   "outputs": [],
   "source": [
    "pandas_df[\"station\"] = pandas_df[\"value\"].str[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3445d5-2e48-4b49-bb27-a65f0d1dc5a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:54.761761Z",
     "iopub.status.busy": "2023-11-22T17:39:54.760650Z",
     "iopub.status.idle": "2023-11-22T17:39:54.766984Z",
     "shell.execute_reply": "2023-11-22T17:39:54.765775Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c89aa78-ea54-4645-bc84-ee1ca068b05b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:54.773313Z",
     "iopub.status.busy": "2023-11-22T17:39:54.772382Z",
     "iopub.status.idle": "2023-11-22T17:39:56.901683Z",
     "shell.execute_reply": "2023-11-22T17:39:56.899333Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>station</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US1WIAD0002  43.9544  -89.8096  294.4 WI ADAMS...</td>\n",
       "      <td>US1WIAD0002</td>\n",
       "      <td>WI ADAMS 0.4 E</td>\n",
       "      <td>ADAMS 0.4 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US1WIAD0005  44.2053  -89.8480  305.7 WI NEKOO...</td>\n",
       "      <td>US1WIAD0005</td>\n",
       "      <td>WI NEKOOSA 8.0 SSE</td>\n",
       "      <td>NEKOOSA 8.0 SSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US1WIAD0006  43.8858  -89.7259  307.8 WI GRAND...</td>\n",
       "      <td>US1WIAD0006</td>\n",
       "      <td>WI GRAND MARSH 1.0 W</td>\n",
       "      <td>GRAND MARSH 1.0 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US1WIAD0008  43.8611  -89.7163  310.0 WI GRAND...</td>\n",
       "      <td>US1WIAD0008</td>\n",
       "      <td>WI GRAND MARSH 1.9 SSW</td>\n",
       "      <td>GRAND MARSH 1.9 SSW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US1WIAD0010  43.7864  -89.6417  293.8 WI OXFOR...</td>\n",
       "      <td>US1WIAD0010</td>\n",
       "      <td>WI OXFORD 4.0 W</td>\n",
       "      <td>OXFORD 4.0 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>USW00094930  43.9333  -90.2667  280.1 WI VOLK ...</td>\n",
       "      <td>USW00094930</td>\n",
       "      <td>WI VOLK FLD ANG</td>\n",
       "      <td>VOLK FLD ANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>USW00094940  43.9667  -90.7333  252.7 WI SPART...</td>\n",
       "      <td>USW00094940</td>\n",
       "      <td>WI SPARTA FT MCCOY</td>\n",
       "      <td>SPARTA FT MCCOY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>USW00094973  46.0303  -91.4425  368.8 WI HAYWA...</td>\n",
       "      <td>USW00094973</td>\n",
       "      <td>WI HAYWARD MUNI AP</td>\n",
       "      <td>HAYWARD MUNI AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>USW00094985  44.6378  -90.1875  381.6 WI MARSH...</td>\n",
       "      <td>USW00094985</td>\n",
       "      <td>WI MARSHFIELD MUNI AP</td>\n",
       "      <td>MARSHFIELD MUNI AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>USW00094994  43.1561  -90.6775  203.0 WI BOSCO...</td>\n",
       "      <td>USW00094994</td>\n",
       "      <td>WI BOSCOBEL AP</td>\n",
       "      <td>BOSCOBEL AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1313 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  value      station  \\\n",
       "0     US1WIAD0002  43.9544  -89.8096  294.4 WI ADAMS...  US1WIAD0002   \n",
       "1     US1WIAD0005  44.2053  -89.8480  305.7 WI NEKOO...  US1WIAD0005   \n",
       "2     US1WIAD0006  43.8858  -89.7259  307.8 WI GRAND...  US1WIAD0006   \n",
       "3     US1WIAD0008  43.8611  -89.7163  310.0 WI GRAND...  US1WIAD0008   \n",
       "4     US1WIAD0010  43.7864  -89.6417  293.8 WI OXFOR...  US1WIAD0010   \n",
       "...                                                 ...          ...   \n",
       "1308  USW00094930  43.9333  -90.2667  280.1 WI VOLK ...  USW00094930   \n",
       "1309  USW00094940  43.9667  -90.7333  252.7 WI SPART...  USW00094940   \n",
       "1310  USW00094973  46.0303  -91.4425  368.8 WI HAYWA...  USW00094973   \n",
       "1311  USW00094985  44.6378  -90.1875  381.6 WI MARSH...  USW00094985   \n",
       "1312  USW00094994  43.1561  -90.6775  203.0 WI BOSCO...  USW00094994   \n",
       "\n",
       "                                         STATE  \\\n",
       "0      WI ADAMS 0.4 E                            \n",
       "1      WI NEKOOSA 8.0 SSE                        \n",
       "2      WI GRAND MARSH 1.0 W                      \n",
       "3      WI GRAND MARSH 1.9 SSW                    \n",
       "4      WI OXFORD 4.0 W                           \n",
       "...                                        ...   \n",
       "1308   WI VOLK FLD ANG                           \n",
       "1309   WI SPARTA FT MCCOY                        \n",
       "1310   WI HAYWARD MUNI AP                        \n",
       "1311   WI MARSHFIELD MUNI AP                     \n",
       "1312   WI BOSCOBEL AP                            \n",
       "\n",
       "                                               NAME  \n",
       "0      ADAMS 0.4 E                                   \n",
       "1      NEKOOSA 8.0 SSE                               \n",
       "2      GRAND MARSH 1.0 W                             \n",
       "3      GRAND MARSH 1.9 SSW                           \n",
       "4      OXFORD 4.0 W                                  \n",
       "...                                             ...  \n",
       "1308   VOLK FLD ANG                                  \n",
       "1309   SPARTA FT MCCOY                               \n",
       "1310   HAYWARD MUNI AP                               \n",
       "1311   MARSHFIELD MUNI AP                            \n",
       "1312   BOSCOBEL AP                                   \n",
       "\n",
       "[1313 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.withColumn(\"station\", expr(\"substring(value, 0, 11)\")).withColumn(\n",
    "     \"STATE\",expr(\"substring(value,38,40)\")).withColumn(\n",
    "     \"NAME\",expr(\"substring(value, 41,71)\"))\n",
    "df2\n",
    "df2=df2.filter(col(\"STATE\").like(\"% WI %\")).filter(col(\"station\").like(\"%US%\"))\n",
    "df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2db2532-d50e-4deb-85c0-59dd54dc1fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:39:56.908204Z",
     "iopub.status.busy": "2023-11-22T17:39:56.907199Z",
     "iopub.status.idle": "2023-11-22T17:40:13.662019Z",
     "shell.execute_reply": "2023-11-22T17:40:13.522447Z"
    }
   },
   "outputs": [],
   "source": [
    "#Filter your results to the state of Wisconsin, collect the rows in your notebook so you can loop over them,\n",
    "#and do an INSERT into your weather.stations table for each station ID and name.\n",
    "insert_query = \"INSERT INTO weather.stations (id, name) VALUES (?, ?)\"\n",
    "prepared_statement = cass.prepare(insert_query)\n",
    "\n",
    "for row in df2.collect():\n",
    "    station_WI = row[\"station\"]\n",
    "    name_WI = row[\"NAME\"]\n",
    "    values = (station_WI, name_WI)\n",
    "    cass.execute(prepared_statement, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1355166c-552d-4820-9de9-5e196da94685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:13.684942Z",
     "iopub.status.busy": "2023-11-22T17:40:13.679729Z",
     "iopub.status.idle": "2023-11-22T17:40:14.915472Z",
     "shell.execute_reply": "2023-11-22T17:40:14.914140Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0   1313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(cass.execute(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) FROM weather.stations\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04cb620-3e0c-43a1-9264-d0c9b8b0fa10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:14.929850Z",
     "iopub.status.busy": "2023-11-22T17:40:14.928923Z",
     "iopub.status.idle": "2023-11-22T17:40:15.165357Z",
     "shell.execute_reply": "2023-11-22T17:40:15.163932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MADISON DANE CO RGNL AP                72641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name\n",
       "0   MADISON DANE CO RGNL AP                72641"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cass.execute(\n",
    "    \"\"\"\n",
    "    SELECT name FROM weather.stations\n",
    "    WHERE id = 'USW00014837'\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aac8906-cdff-4627-8cfb-1f57e8a3767a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:15.170799Z",
     "iopub.status.busy": "2023-11-22T17:40:15.170276Z",
     "iopub.status.idle": "2023-11-22T17:40:15.175498Z",
     "shell.execute_reply": "2023-11-22T17:40:15.173989Z"
    }
   },
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d4b5bb-3a01-457e-8a99-ec1bbe4fcf83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:15.180881Z",
     "iopub.status.busy": "2023-11-22T17:40:15.179773Z",
     "iopub.status.idle": "2023-11-22T17:40:15.245662Z",
     "shell.execute_reply": "2023-11-22T17:40:15.244314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_token_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9014250178872933741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       system_token_id\n",
       "0 -9014250178872933741"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the token for the station\n",
    "token_df = pd.DataFrame(cass.execute(\"SELECT TOKEN(id) FROM weather.stations WHERE id = 'USC00470273'\"))\n",
    "token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20774efa-14bb-412e-bde1-8b7d6778ee97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:15.255242Z",
     "iopub.status.busy": "2023-11-22T17:40:15.254380Z",
     "iopub.status.idle": "2023-11-22T17:40:15.260112Z",
     "shell.execute_reply": "2023-11-22T17:40:15.258974Z"
    }
   },
   "outputs": [],
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43605538-6981-4030-8738-4a97f451c732",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:26.274699Z",
     "iopub.status.busy": "2023-11-22T17:40:26.274120Z",
     "iopub.status.idle": "2023-11-22T17:40:28.781003Z",
     "shell.execute_reply": "2023-11-22T17:40:28.779548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8715368926633190629"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get the token for the station\n",
    "station_token = cass.execute(\"SELECT TOKEN(id) FROM weather.stations WHERE id = 'USC00470273'\").one()[0]\n",
    "\n",
    "# Run nodetool ring and get the output\n",
    "output = subprocess.check_output([\"nodetool\", \"ring\"]).decode()\n",
    "\n",
    "# Split the output into lines\n",
    "lines = output.split(\"\\n\")\n",
    "\n",
    "# Initialize the smallest token difference and the next token\n",
    "smallest_diff = float(\"inf\")\n",
    "next_token = None\n",
    "\n",
    "# Loop over the lines\n",
    "for line in lines:\n",
    "    # Split the line into columns\n",
    "    columns = line.split()\n",
    "    \n",
    "    # Skip the line if it doesn't have enough columns\n",
    "    if len(columns) < 8:\n",
    "        continue\n",
    "    \n",
    "    # Try to get the token from the line\n",
    "    try:\n",
    "        token = int(columns[7])\n",
    "    except ValueError:\n",
    "        # Skip this line if the token is not an integer\n",
    "        continue\n",
    "    \n",
    "    # Calculate the difference between the station token and this token\n",
    "    diff = token - station_token\n",
    "    \n",
    "    # Handle the case where the ring \"wraps around\"\n",
    "    if diff < 0:\n",
    "        diff += 2**64\n",
    "    \n",
    "    # If this difference is smaller than the smallest difference we've seen so far,\n",
    "    # update the smallest difference and the next token\n",
    "    if diff < smallest_diff:\n",
    "        smallest_diff = diff\n",
    "        next_token = token\n",
    "\n",
    "next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82f79960-81cb-4e80-84a9-5cc26238b74d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:28.787223Z",
     "iopub.status.busy": "2023-11-22T17:40:28.786186Z",
     "iopub.status.idle": "2023-11-22T17:40:28.791649Z",
     "shell.execute_reply": "2023-11-22T17:40:28.790356Z"
    }
   },
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ac597df-da4d-4afc-a898-1eb4bef714c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:28.796484Z",
     "iopub.status.busy": "2023-11-22T17:40:28.795900Z",
     "iopub.status.idle": "2023-11-22T17:40:29.214040Z",
     "shell.execute_reply": "2023-11-22T17:40:29.213128Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Protocol message StationMaxRequest has no \"id\" field.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m stub \u001b[38;5;241m=\u001b[39m station_pb2_grpc\u001b[38;5;241m.\u001b[39mStationStub(channel)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Make a StationMax request\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m stub\u001b[38;5;241m.\u001b[39mStationMax(\u001b[43mstation_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStationMaxRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUSW00014837\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m response\u001b[38;5;241m.\u001b[39mmax\n",
      "\u001b[0;31mValueError\u001b[0m: Protocol message StationMaxRequest has no \"id\" field."
     ]
    }
   ],
   "source": [
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "import grpc\n",
    "\n",
    "# Create a gRPC channel and stub\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "# Make a StationMax request\n",
    "response = stub.StationMax(station_pb2.StationMaxRequest(id='USW00014837'))\n",
    "\n",
    "response.max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "101a2bf7-594d-47e2-bf3a-213ebe6aeb71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:29.219168Z",
     "iopub.status.busy": "2023-11-22T17:40:29.218481Z",
     "iopub.status.idle": "2023-11-22T17:40:29.223629Z",
     "shell.execute_reply": "2023-11-22T17:40:29.222407Z"
    }
   },
   "outputs": [],
   "source": [
    "#q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fda9dd79-9433-4d1a-a91b-e9f276264e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:29.228078Z",
     "iopub.status.busy": "2023-11-22T17:40:29.227533Z",
     "iopub.status.idle": "2023-11-22T17:40:34.427491Z",
     "shell.execute_reply": "2023-11-22T17:40:34.424288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\") \\\n",
    "    .option(\"keyspace\", \"weather\") \\\n",
    "    .option(\"table\", \"stations\") \\\n",
    "    .load()\n",
    "\n",
    "# Create a temporary view named 'stations'\n",
    "df.createOrReplaceTempView(\"stations\")\n",
    "\n",
    "# List the tables/views in the Spark catalog\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b59373a-ee0f-4e2d-8fd4-dbab29c2b8ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:34.434937Z",
     "iopub.status.busy": "2023-11-22T17:40:34.433889Z",
     "iopub.status.idle": "2023-11-22T17:40:34.447801Z",
     "shell.execute_reply": "2023-11-22T17:40:34.445995Z"
    }
   },
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45571063-af1c-439a-b22f-bfce0dc0cd7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:41:00.490146Z",
     "iopub.status.busy": "2023-11-22T17:41:00.488995Z",
     "iopub.status.idle": "2023-11-22T17:41:10.045809Z",
     "shell.execute_reply": "2023-11-22T17:41:10.044590Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 17:41:01 ERROR CassandraConnectorConf: Unknown host 'p6-db-2'\n",
      "java.net.UnknownHostException: p6-db-2: Temporary failure in name resolution\n",
      "\tat java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:867)\n",
      "\tat java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1302)\n",
      "\tat java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:815)\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n",
      "\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.maybeResolveHostAndPort(CassandraConnectorConf.scala:346)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.$anonfun$getIpBasedContactInfoFromSparkConf$1(CassandraConnectorConf.scala:379)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.immutable.Set$Set3.foreach(Set.scala:233)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.getIpBasedContactInfoFromSparkConf(CassandraConnectorConf.scala:378)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.getContactInfoFromSparkConf(CassandraConnectorConf.scala:368)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.fromSparkConf(CassandraConnectorConf.scala:433)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.apply(CassandraConnectorConf.scala:358)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnector$.apply(CassandraConnector.scala:225)\n",
      "\tat com.datastax.spark.connector.datasource.CassandraScanBuilder.<init>(CassandraScanBuilder.scala:41)\n",
      "\tat com.datastax.spark.connector.datasource.CassandraTable.newScanBuilder(CassandraTable.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$$anonfun$createScanBuilder$1.applyOrElse(V2ScanRelationPushDown.scala:57)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$$anonfun$createScanBuilder$1.applyOrElse(V2ScanRelationPushDown.scala:55)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:300)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1122)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.createScanBuilder(V2ScanRelationPushDown.scala:55)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.$anonfun$apply$1(V2ScanRelationPushDown.scala:42)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.$anonfun$apply$8(V2ScanRelationPushDown.scala:51)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.apply(V2ScanRelationPushDown.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.apply(V2ScanRelationPushDown.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:153)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/22 17:41:06 WARN ChannelPool: [s1|/172.22.0.2:9042]  Error while opening new channel (ConnectionInitException: [s1|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=6eb8d507-0d60-4560-88e6-4f87b11640f6, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700674778941}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "23/11/22 17:41:06 ERROR CassandraConnectorConf: Unknown host 'p6-db-2'\n",
      "java.net.UnknownHostException: p6-db-2\n",
      "\tat java.net.InetAddress$CachedAddresses.get(InetAddress.java:764)\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1291)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1144)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1065)\n",
      "\tat java.net.InetAddress.getByName(InetAddress.java:1015)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.maybeResolveHostAndPort(CassandraConnectorConf.scala:346)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.$anonfun$getIpBasedContactInfoFromSparkConf$1(CassandraConnectorConf.scala:379)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.immutable.Set$Set3.foreach(Set.scala:233)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.getIpBasedContactInfoFromSparkConf(CassandraConnectorConf.scala:378)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.getContactInfoFromSparkConf(CassandraConnectorConf.scala:368)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.fromSparkConf(CassandraConnectorConf.scala:433)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnectorConf$.apply(CassandraConnectorConf.scala:358)\n",
      "\tat com.datastax.spark.connector.cql.CassandraConnector$.apply(CassandraConnector.scala:225)\n",
      "\tat org.apache.spark.sql.cassandra.SolrPredicateRules.apply(SolrPredicateRules.scala:57)\n",
      "\tat org.apache.spark.sql.cassandra.SolrPredicateRules.apply(SolrPredicateRules.scala:94)\n",
      "\tat com.datastax.spark.connector.datasource.CassandraScanBuilder.$anonfun$pushFilters$4(CassandraScanBuilder.scala:83)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat com.datastax.spark.connector.datasource.CassandraScanBuilder.pushFilters(CassandraScanBuilder.scala:82)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PushDownUtils$.pushFilters(PushDownUtils.scala:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$$anonfun$pushDownFilters$1.applyOrElse(V2ScanRelationPushDown.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$$anonfun$pushDownFilters$1.applyOrElse(V2ScanRelationPushDown.scala:60)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1122)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:517)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.pushDownFilters(V2ScanRelationPushDown.scala:60)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.$anonfun$apply$3(V2ScanRelationPushDown.scala:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.$anonfun$apply$8(V2ScanRelationPushDown.scala:51)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.apply(V2ScanRelationPushDown.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2ScanRelationPushDown$.apply(V2ScanRelationPushDown.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:153)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:171)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 2) / 6]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:===================>                                       (2 + 2) / 6]\r",
      "\r",
      "[Stage 5:=======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the average difference between tmax and tmin for each station\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT id, AVG(record.tmax - record.tmin) as avg_diff\n",
    "FROM stations\n",
    "WHERE record.tmax IS NOT NULL AND record.tmin IS NOT NULL\n",
    "GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "# Convert the result to a dictionary\n",
    "result_dict = {row.id: row.avg_diff for row in result.collect()}\n",
    "\n",
    "result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86e34fbb-12bb-43e1-aed4-9a2d031769e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:41:10.063664Z",
     "iopub.status.busy": "2023-11-22T17:41:10.057365Z",
     "iopub.status.idle": "2023-11-22T17:41:10.075129Z",
     "shell.execute_reply": "2023-11-22T17:41:10.073758Z"
    }
   },
   "outputs": [],
   "source": [
    "#q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "321cbd6f-8f4a-4470-8c5a-6878094343d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:41:10.084182Z",
     "iopub.status.busy": "2023-11-22T17:41:10.081496Z",
     "iopub.status.idle": "2023-11-22T17:41:10.260421Z",
     "shell.execute_reply": "2023-11-22T17:41:10.258750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: docker: not found\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it p6-db-1 nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "485abf52-eea2-407c-b4ab-e309f3b40bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:41:10.267242Z",
     "iopub.status.busy": "2023-11-22T17:41:10.266023Z",
     "iopub.status.idle": "2023-11-22T17:41:10.271718Z",
     "shell.execute_reply": "2023-11-22T17:41:10.270863Z"
    }
   },
   "outputs": [],
   "source": [
    "#q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3491333d-5b99-475c-ad07-941c142fdfaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:41:10.276282Z",
     "iopub.status.busy": "2023-11-22T17:41:10.275374Z",
     "iopub.status.idle": "2023-11-22T17:41:10.312263Z",
     "shell.execute_reply": "2023-11-22T17:41:10.311150Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Protocol message StationMaxRequest has no \"id\" field.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m stub \u001b[38;5;241m=\u001b[39m station_pb2_grpc\u001b[38;5;241m.\u001b[39mStationStub(channel)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Make a StationMax request\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m stub\u001b[38;5;241m.\u001b[39mStationMax(\u001b[43mstation_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStationMaxRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUSW00014837\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m response\u001b[38;5;241m.\u001b[39merror\n",
      "\u001b[0;31mValueError\u001b[0m: Protocol message StationMaxRequest has no \"id\" field."
     ]
    }
   ],
   "source": [
    "# Create a gRPC channel and stub\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "# Make a StationMax request\n",
    "response = stub.StationMax(station_pb2.StationMaxRequest(id='USW00014837'))\n",
    "\n",
    "response.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca0ca893-454b-4e95-9a38-96d116484d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:41:10.318538Z",
     "iopub.status.busy": "2023-11-22T17:41:10.317211Z",
     "iopub.status.idle": "2023-11-22T17:41:10.323133Z",
     "shell.execute_reply": "2023-11-22T17:41:10.321817Z"
    }
   },
   "outputs": [],
   "source": [
    "#q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3f65693-2145-4eca-8642-df9311efabd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T17:41:10.328340Z",
     "iopub.status.busy": "2023-11-22T17:41:10.327914Z",
     "iopub.status.idle": "2023-11-22T17:41:10.359714Z",
     "shell.execute_reply": "2023-11-22T17:41:10.358714Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'station_pb2' has no attribute 'TemperatureRecord'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m stub \u001b[38;5;241m=\u001b[39m station_pb2_grpc\u001b[38;5;241m.\u001b[39mStationStub(channel)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a valid request message\u001b[39;00m\n\u001b[1;32m      7\u001b[0m record_temps_request \u001b[38;5;241m=\u001b[39m station_pb2\u001b[38;5;241m.\u001b[39mRecordTempsRequest(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSW00014837\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     records\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 10\u001b[0m         \u001b[43mstation_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTemperatureRecord\u001b[49m(date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-11-21\u001b[39m\u001b[38;5;124m'\u001b[39m, tmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, tmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m),\n\u001b[1;32m     11\u001b[0m         station_pb2\u001b[38;5;241m.\u001b[39mTemperatureRecord(date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-11-22\u001b[39m\u001b[38;5;124m'\u001b[39m, tmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, tmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m),\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Make the call\u001b[39;00m\n\u001b[1;32m     16\u001b[0m response \u001b[38;5;241m=\u001b[39m stub\u001b[38;5;241m.\u001b[39mRecordTemps(record_temps_request)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'station_pb2' has no attribute 'TemperatureRecord'"
     ]
    }
   ],
   "source": [
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "\n",
    "# Create a stub (client)\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "# Create a valid request message\n",
    "record_temps_request = station_pb2.RecordTempsRequest(\n",
    "    id='USW00014837',\n",
    "    records=[\n",
    "        station_pb2.TemperatureRecord(date='2023-11-21', tmin=10, tmax=20),\n",
    "        station_pb2.TemperatureRecord(date='2023-11-22', tmin=15, tmax=25),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Make the call\n",
    "response = stub.RecordTemps(record_temps_request)\n",
    "\n",
    "response.error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05a53e-9a4e-4488-8fa9-9cbc94b89329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
