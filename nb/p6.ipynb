{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad81a6d-1fed-499c-9c95-fb69e89a7f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address       Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.16.2  25.55 KiB   16      63.8%             706974e1-7c77-4288-a9a2-568e1ade462a  rack1\n",
      "UN  192.168.16.3  25.55 KiB   16      70.9%             94a92f53-acc1-448a-9c99-7f7aa3123eb6  rack1\n",
      "UN  192.168.16.4  242.42 KiB  16      65.3%             b235d3cd-d3ba-4b7c-8d3f-0149a2e55ac9  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a20b2c-e91a-43ec-b803-5ab403442b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to the Cassandra cluster\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d45fbfa-9090-4390-aede-7cd3289acfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE weather.stations (\n",
      "    id text,\n",
      "    date date,\n",
      "    name text static,\n",
      "    record station_record,\n",
      "    PRIMARY KEY (id, date)\n",
      ") WITH CLUSTERING ORDER BY (date ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND cdc = false\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND memtable = 'default'\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND extensions = {}\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()\n",
    "\n",
    "cass.execute(\"DROP KEYSPACE IF EXISTS weather\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE KEYSPACE weather \n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}\n",
    "\"\"\")\n",
    "\n",
    "cass.set_keyspace('weather')\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TYPE station_record (\n",
    "        tmin int,\n",
    "        tmax int\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TABLE stations (\n",
    "        id text,\n",
    "        name text static,\n",
    "        date date,\n",
    "        record station_record,\n",
    "        PRIMARY KEY (id, date)\n",
    "    ) WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")\n",
    "\n",
    "print(cass.execute(\"describe table weather.stations \").one().create_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c14cbff-ca2d-47d4-88fa-52dd1125fd3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a999bfb1-118a-4d81-afdb-2f8899eb7f32;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (188ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (83ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (341ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (60ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (152ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (38ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (112ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (44ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (31ms)\n",
      ":: resolution report :: resolve 6349ms :: artifacts dl 1374ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a999bfb1-118a-4d81-afdb-2f8899eb7f32\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/184ms)\n",
      "23/11/22 22:21:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c0e3fe-3ad3-49e8-b1be-ae0e4d8194a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())\n",
    "\n",
    "stations = spark.read.text(\"ghcnd-stations.txt\")\n",
    "\n",
    "stations = stations.select(\n",
    "    substring(stations['value'], 1, 11).alias('id'),\n",
    "    substring(stations['value'], 39, 2).alias('state'),\n",
    "    substring(stations['value'], 42, 30).alias('name')\n",
    ")\n",
    "\n",
    "stations = stations.filter(stations['state'] == 'WI')\n",
    "\n",
    "for row in stations.collect():\n",
    "    cass.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO stations (id, name)\n",
    "        VALUES (%s, %s)\n",
    "        \"\"\",\n",
    "        (row['id'], row['name'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4ba10d-08c5-4e98-b95b-86cde5e0cce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313\n"
     ]
    }
   ],
   "source": [
    "rows = cass.execute(\"SELECT COUNT(*) FROM weather.stations\")\n",
    "\n",
    "for row in rows:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748c6acf-3a0e-4247-85f6-73e09d4e1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MADISON DANE CO RGNL AP       \n"
     ]
    }
   ],
   "source": [
    "#q2\n",
    "rows = cass.execute(\"SELECT name FROM stations WHERE id = 'USW00014837'\")\n",
    "\n",
    "for row in rows:\n",
    "    print(row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b03d9d1-8142-4b66-aad7-329ad064b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9014250178872933741\n"
     ]
    }
   ],
   "source": [
    "#q3\n",
    "rows = cass.execute(\"SELECT TOKEN(id) FROM stations WHERE id = 'USC00470273'\")\n",
    "\n",
    "for row in rows:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81fcac1e-12b9-412a-87cc-42a1dc1a3010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134/1313732839.py:2: DeprecationWarning: ResultSet indexing support will be removed in 4.0. Consider using ResultSet.one() to get a single row.\n",
      "  usc_token = rows[0][0]\n"
     ]
    }
   ],
   "source": [
    "rows = cass.execute(\"SELECT TOKEN(id) FROM stations WHERE id = 'USC00470273'\")\n",
    "usc_token = rows[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43cd97e7-8ba9-4d9c-baa2-3ffb8c59aeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usc_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27b039a9-3c75-4e9b-8ba3-d76684eef3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8945481626543146300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "import subprocess\n",
    "\n",
    "# Get the token for USC00470273\n",
    "rows = cass.execute(\"SELECT TOKEN(id) FROM stations WHERE id = 'USC00470273'\")\n",
    "usc_token = rows.one()[0]\n",
    "\n",
    "# Run nodetool ring\n",
    "output = subprocess.check_output([\"nodetool\", \"ring\"]).decode()\n",
    "\n",
    "# Parse the output\n",
    "lines = output.split(\"\\n\")[4:-1]  # Remove the header and footer\n",
    "\n",
    "# Extract valid tokens from the lines\n",
    "tokens = []\n",
    "for line in lines:\n",
    "    split_line = line.split()\n",
    "    if len(split_line) > 7:\n",
    "        token_str = split_line[7]\n",
    "        try:\n",
    "            token = int(token_str)\n",
    "            tokens.append(token)\n",
    "        except ValueError:\n",
    "            continue\n",
    "# Sort the tokens\n",
    "tokens.sort()\n",
    "\n",
    "# Find the token that comes after the token for USC00470273\n",
    "next_token = None\n",
    "for token in tokens:\n",
    "    if token >= usc_token:\n",
    "        next_token = token\n",
    "        break\n",
    "\n",
    "# Handle the case where the ring \"wraps around\"\n",
    "if next_token is None and tokens:\n",
    "    next_token = tokens[0]\n",
    "\n",
    "next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ba1f2-51da-4880-8190-7ce0c0840c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations=cass.execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95031c07-4181-4018-befa-c4defc6a01fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  records.zip\n",
      "   creating: records.parquet/\n",
      "  inflating: records.parquet/part-00000-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet  \n",
      " extracting: records.parquet/._SUCCESS.crc  \n",
      "  inflating: records.parquet/part-00002-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet  \n",
      "  inflating: records.parquet/part-00001-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet  \n",
      "  inflating: records.parquet/part-00003-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet  \n",
      " extracting: records.parquet/.part-00003-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet.crc  \n",
      " extracting: records.parquet/_SUCCESS  \n",
      " extracting: records.parquet/.part-00000-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet.crc  \n",
      " extracting: records.parquet/.part-00001-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet.crc  \n",
      " extracting: records.parquet/.part-00002-574ab704-2431-4c8b-9d88-6c635a467b99-c000.snappy.parquet.crc  \n"
     ]
    }
   ],
   "source": [
    "!unzip records.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0845c093-b84f-4bdc-ade3-ecbf98bcdf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+------+\n",
      "|    station|    date|element| value|\n",
      "+-----------+--------+-------+------+\n",
      "|USW00014898|20220101|   TMAX| -32.0|\n",
      "|USW00014898|20220102|   TMAX| -77.0|\n",
      "|USW00014898|20220103|   TMAX| -60.0|\n",
      "|USW00014898|20220104|   TMAX|   0.0|\n",
      "|USW00014898|20220105|   TMAX| -16.0|\n",
      "|USW00014898|20220106|   TMAX| -71.0|\n",
      "|USW00014898|20220107|   TMAX| -71.0|\n",
      "|USW00014898|20220108|   TMAX| -32.0|\n",
      "|USW00014898|20220109|   TMAX| -27.0|\n",
      "|USW00014898|20220110|   TMAX|-149.0|\n",
      "|USW00014898|20220111|   TMAX| -16.0|\n",
      "|USW00014898|20220112|   TMAX|   6.0|\n",
      "|USW00014898|20220113|   TMAX|  11.0|\n",
      "|USW00014898|20220114|   TMAX| -77.0|\n",
      "|USW00014898|20220115|   TMAX| -99.0|\n",
      "|USW00014898|20220116|   TMAX| -60.0|\n",
      "|USW00014898|20220117|   TMAX| -21.0|\n",
      "|USW00014898|20220118|   TMAX|  28.0|\n",
      "|USW00014898|20220119|   TMAX|  28.0|\n",
      "|USW00014898|20220120|   TMAX|-121.0|\n",
      "+-----------+--------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"WeatherDataProcessing\").getOrCreate()\n",
    "\n",
    "# Assuming records.parquet is the path to your Parquet files\n",
    "parquet_path = \"records.parquet\"\n",
    "df = spark.read.parquet(parquet_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc504d0-6ef3-4c76-87d0-7cf13b750c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f51540dc-08f3-410a-8b70-767f0b4220fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['station', 'date', 'element', 'value']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df67fb3d-a164-4672-ab6a-7a2e4f8ca9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+\n",
      "|    station|    date|  TMIN|  TMAX|\n",
      "+-----------+--------+------+------+\n",
      "|USW00014898|20220107|-166.0| -71.0|\n",
      "|USW00014839|20220924| 117.0| 194.0|\n",
      "|USW00014839|20220523|  83.0| 150.0|\n",
      "|USW00014839|20221019|  11.0|  83.0|\n",
      "|USW00014839|20220529| 139.0| 261.0|\n",
      "|USR0000WDDG|20221130|-106.0| -39.0|\n",
      "|USR0000WDDG|20220119|-178.0| -56.0|\n",
      "|USW00014837|20220222| -88.0| -38.0|\n",
      "|USR0000WDDG|20220202|-150.0|-106.0|\n",
      "|USW00014839|20220427|   0.0|  39.0|\n",
      "|USW00014839|20220708| 189.0| 222.0|\n",
      "|USW00014839|20220917| 200.0| 294.0|\n",
      "|USW00014837|20220624| 200.0| 322.0|\n",
      "|USW00014898|20220129|-116.0| -60.0|\n",
      "|USW00014839|20220715| 156.0| 233.0|\n",
      "|USR0000WDDG|20220224|-128.0| -61.0|\n",
      "|USR0000WDDG|20220130|-117.0| -33.0|\n",
      "|USR0000WDDG|20220414| -17.0|  50.0|\n",
      "|USW00014898|20220728| 156.0| 256.0|\n",
      "|USW00014837|20220906| 117.0| 256.0|\n",
      "+-----------+--------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pivoted_df = df.groupBy(\"station\", \"date\").pivot(\"element\").agg({\"value\": \"first\"})\n",
    "\n",
    "# Rename the columns to remove the 'value_' prefix\n",
    "pivoted_df = pivoted_df.withColumnRenamed(\"value_tmin\", \"TMIN\").withColumnRenamed(\"value_tmax\", \"TMAX\")\n",
    "\n",
    "# Select relevant columns\n",
    "result_df = pivoted_df.select(\"station\", \"date\", \"TMIN\", \"TMAX\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "result_df.show()\n",
    "\n",
    "# Save the rearranged DataFrame to a new Parquet file if needed\n",
    "result_df.write.parquet(\"path/to/rearranged_data.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb5c09ce-f2b1-4b11-8d05-a97ff3e2a72a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'Column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result_row \u001b[38;5;129;01min\u001b[39;00m result_df:\n\u001b[1;32m      2\u001b[0m     res\u001b[38;5;241m=\u001b[39mstub\u001b[38;5;241m.\u001b[39mRecordTemps(station_pb2\u001b[38;5;241m.\u001b[39mRecordTempsRequest(\n\u001b[1;32m      3\u001b[0m         station\u001b[38;5;241m=\u001b[39mresult_row\u001b[38;5;241m.\u001b[39mstation,\n\u001b[1;32m      4\u001b[0m         date\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(result_row\u001b[38;5;241m.\u001b[39mdate),\n\u001b[0;32m----> 5\u001b[0m         tmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult_row\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTMIN\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      6\u001b[0m         tmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(result_row\u001b[38;5;241m.\u001b[39mTMAX)\n\u001b[1;32m      7\u001b[0m     ))\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'Column'"
     ]
    }
   ],
   "source": [
    "for result_row in result_df:\n",
    "    res=stub.RecordTemps(station_pb2.RecordTempsRequest(\n",
    "        station=result_row.station,\n",
    "        date=str(result_row.date),\n",
    "        tmin=int(result_row.TMIN),\n",
    "        tmax=int(result_row.TMAX)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7706e2-0f3a-4bf1-a0e1-9b5abb502a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925d0d9-7f85-49f1-a225-12727f67d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for in df:\n",
    "    Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11e67def-3a55-43d1-91e9-b04d1cbe5174",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {created_time:\"2023-11-22T23:35:20.586602417+00:00\", grpc_status:14}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m station_max_request \u001b[38;5;241m=\u001b[39m station_pb2\u001b[38;5;241m.\u001b[39mStationMaxRequest(station\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSW00014837\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Make the call\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStationMax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_max_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m response\u001b[38;5;241m.\u001b[39mtmax\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1148\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1155\u001b[0m     (\n\u001b[1;32m   1156\u001b[0m         state,\n\u001b[1;32m   1157\u001b[0m         call,\n\u001b[1;32m   1158\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1159\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[0;32m-> 1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {created_time:\"2023-11-22T23:35:20.586602417+00:00\", grpc_status:14}\"\n>"
     ]
    }
   ],
   "source": [
    "#q5\n",
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "\n",
    "# Open a gRPC channel\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "\n",
    "# Create a stub (client)\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "\n",
    "\n",
    "# Create a valid request message\n",
    "station_max_request = station_pb2.StationMaxRequest(station='USW00014837')\n",
    "\n",
    "# Make the call\n",
    "response = stub.StationMax(station_max_request)\n",
    "\n",
    "response.tmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5760d511-aea0-4244-849d-50d8e5f18db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that corresponds to the stations table in Cassandra\n",
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\\\n",
    "    .option(\"keyspace\", \"weather\")\\\n",
    "    .option(\"table\", \"stations\")\\\n",
    "    .load()\n",
    "\n",
    "# Create a temporary view named stations\n",
    "df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30df610e-f4f1-4d7c-92d9-44fd0ccc1afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stations\n"
     ]
    }
   ],
   "source": [
    "#q6\n",
    "# List the tables/views available in the Spark catalog\n",
    "tables = spark.catalog.listTables()\n",
    "\n",
    "# Print the tables/views\n",
    "for table in tables:\n",
    "    print(table.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8b4ed3b-2dd3-4bb0-a8ed-5b379de4b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the average difference between tmax and tmin for each station\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT id, AVG(record.tmax - record.tmin) as avg_diff\n",
    "    FROM stations\n",
    "    WHERE record.tmax IS NOT NULL AND record.tmin IS NOT NULL\n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "# Convert the result to a dictionary\n",
    "result_dict = {row['id']: row['avg_diff'] for row in result.collect()}\n",
    "\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bc8bef6-4a07-4870-89f9-fc8a08cd7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: docker: not found\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!docker exec -it p6-db-1 nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a61e572e-7a2a-4783-a475-7d44078bff49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b00a7f78-6792-4d1a-b70d-c59ec9b3a262",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2023-11-22T20:09:46.038250423+00:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m station_max_request \u001b[38;5;241m=\u001b[39m station_pb2\u001b[38;5;241m.\u001b[39mStationMaxRequest(station\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSW00014837\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make the call\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStationMax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_max_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m response\u001b[38;5;241m.\u001b[39merror\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1148\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1155\u001b[0m     (\n\u001b[1;32m   1156\u001b[0m         state,\n\u001b[1;32m   1157\u001b[0m         call,\n\u001b[1;32m   1158\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1159\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[0;32m-> 1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2023-11-22T20:09:46.038250423+00:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "#q9\n",
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "\n",
    "# Open a gRPC channel\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "\n",
    "# Create a stub (client)\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "# Create a valid request message\n",
    "station_max_request = station_pb2.StationMaxRequest(station='USW00014837')\n",
    "\n",
    "# Make the call\n",
    "response = stub.StationMax(station_max_request)\n",
    "\n",
    "response.error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ac597df-da4d-4afc-a898-1eb4bef714c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2023-11-22T20:12:17.753643897+00:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m record_temps_request \u001b[38;5;241m=\u001b[39m station_pb2\u001b[38;5;241m.\u001b[39mRecordTempsRequest(\n\u001b[1;32m      9\u001b[0m     station\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSW00014837\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-11-22\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     tmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     12\u001b[0m     tmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Make the call\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecordTemps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord_temps_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m response\u001b[38;5;241m.\u001b[39merror\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1148\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1155\u001b[0m     (\n\u001b[1;32m   1156\u001b[0m         state,\n\u001b[1;32m   1157\u001b[0m         call,\n\u001b[1;32m   1158\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1159\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[0;32m-> 1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2023-11-22T20:12:17.753643897+00:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "#q10\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "\n",
    "# Create a stub (client)\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "# Create a valid request message\n",
    "record_temps_request = station_pb2.RecordTempsRequest(\n",
    "    id='USW00014837',\n",
    "    records=[\n",
    "        station_pb2.TemperatureRecord(date='2023-11-21', tmin=10, tmax=20),\n",
    "        station_pb2.TemperatureRecord(date='2023-11-22', tmin=15, tmax=25),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Make the call\n",
    "response = stub.RecordTemps(record_temps_request)\n",
    "\n",
    "response.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05a53e-9a4e-4488-8fa9-9cbc94b89329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
